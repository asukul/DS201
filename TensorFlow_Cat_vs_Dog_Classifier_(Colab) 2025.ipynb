{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asukul/DS201/blob/master/TensorFlow_Cat_vs_Dog_Classifier_(Colab)%202025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install tensorflow"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aiBe7iGg4lYu",
        "outputId": "0c0773b7-8be2-439b-c133-690031261b28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Cat vs Dog Image Classifier using TensorFlow in Colab.\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Loading the cats_vs_dogs dataset from TFDS.\n",
        "2. Building and training a basic CNN.\n",
        "3. Visualizing results (plots, TensorBoard) and identifying overfitting.\n",
        "4. Applying data augmentation and dropout.\n",
        "5. Training an improved model and visualizing its results.\n",
        "\"\"\"\n",
        "\n",
        "# @title # 1. Setup and Imports\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# Helper function to plot accuracy and loss curves\n",
        "def plot_history(history, title_prefix=\"\"):\n",
        "    \"\"\"Plots training and validation accuracy/loss.\"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
        "    plt.title(f'{title_prefix}Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
        "    plt.title(f'{title_prefix}Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Helper function to display sample predictions\n",
        "def show_sample_predictions(dataset, model, class_names, num_samples=16):\n",
        "    \"\"\"Shows images with their predicted and true labels.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    # Take one batch from the dataset\n",
        "    for images, labels in dataset.take(1):\n",
        "        predictions = model.predict(images)\n",
        "        # Squeeze predictions if necessary (e.g., if output shape is (batch, 1))\n",
        "        if predictions.shape[-1] == 1:\n",
        "            predictions = tf.squeeze(predictions, axis=-1)\n",
        "\n",
        "        # Convert predictions to labels (0 or 1) based on threshold 0.5\n",
        "        predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "        for i in range(min(num_samples, images.shape[0])):\n",
        "            ax = plt.subplot(4, 4, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            true_label = class_names[labels[i]]\n",
        "            pred_label = class_names[predicted_labels[i]]\n",
        "            confidence = predictions[i] if predicted_labels[i] == 1 else 1 - predictions[i]\n",
        "\n",
        "            plt.title(f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}\",\n",
        "                      color=(\"green\" if pred_label == true_label else \"red\"))\n",
        "            plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# @title # 2. Load and Prepare the Dataset\n",
        "\n",
        "# Load the cats_vs_dogs dataset\n",
        "# It contains images of cats and dogs, split into training set.\n",
        "# We'll split the training set further into training and validation.\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], # 80% train, 10% val, 10% test\n",
        "    with_info=True,\n",
        "    as_supervised=True, # Returns (image, label) tuples\n",
        ")\n",
        "\n",
        "print(\"Raw Training Examples:\", tf.data.experimental.cardinality(raw_train))\n",
        "print(\"Raw Validation Examples:\", tf.data.experimental.cardinality(raw_validation))\n",
        "print(\"Raw Test Examples:\", tf.data.experimental.cardinality(raw_test))\n",
        "\n",
        "# Class names (0: cat, 1: dog - based on TFDS documentation)\n",
        "CLASS_NAMES = ['cat', 'dog']\n",
        "\n",
        "# Define image size and batch size\n",
        "IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Preprocessing function: resize and normalize images\n",
        "def format_example(image, label):\n",
        "    \"\"\"Resizes image to IMG_SIZE x IMG_SIZE and normalizes pixels to [0, 1].\"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 255.0) # Normalize pixel values to [0, 1]\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing to datasets\n",
        "train_dataset = raw_train.map(format_example)\n",
        "validation_dataset = raw_validation.map(format_example)\n",
        "test_dataset = raw_test.map(format_example)\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "# AUTOTUNE allows TensorFlow to find the best allocation of CPU resources.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_batches = train_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "validation_batches = validation_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "test_batches = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Verify the shape of the data\n",
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    pass\n",
        "print(\"Batch shape:\", image_batch.shape, label_batch.shape)\n",
        "\n",
        "# @title # 3. Build the Initial CNN Model\n",
        "# A simple stack of Conv2D and MaxPooling2D layers\n",
        "\n",
        "initial_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)), # Define input shape explicitly\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # Output layer: Dense with 1 unit (binary classification) and sigmoid activation\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss='binary_crossentropy', # Suitable for binary (0/1) classification\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "initial_model.summary()\n",
        "\n",
        "# @title # 4. Train the Initial Model\n",
        "# Setup TensorBoard logs\n",
        "log_dir_initial = os.path.join(\"logs\", \"initial_fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback_initial = tf.keras.callbacks.TensorBoard(log_dir=log_dir_initial, histogram_freq=1)\n",
        "\n",
        "# Define number of epochs\n",
        "EPOCHS_INITIAL = 15 # Adjust as needed, 15 is a starting point\n",
        "\n",
        "print(\"\\n--- Training Initial Model ---\")\n",
        "history_initial = initial_model.fit(\n",
        "    train_batches,\n",
        "    epochs=EPOCHS_INITIAL,\n",
        "    validation_data=validation_batches,\n",
        "    callbacks=[tensorboard_callback_initial]\n",
        ")\n",
        "print(\"--- Initial Model Training Complete ---\")\n",
        "\n",
        "# @title # 5. Evaluate the Initial Model\n",
        "\n",
        "print(\"\\n--- Evaluating Initial Model ---\")\n",
        "# Plot training history\n",
        "plot_history(history_initial, title_prefix=\"Initial Model: \")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"Sample Predictions from Initial Model (on Test Set):\")\n",
        "# Use test_batches for showing predictions on unseen data\n",
        "show_sample_predictions(test_batches, initial_model, CLASS_NAMES)\n",
        "\n",
        "# Evaluate on the test set\n",
        "loss_initial, accuracy_initial = initial_model.evaluate(test_batches)\n",
        "print(f\"\\nInitial Model Test Loss: {loss_initial:.4f}\")\n",
        "print(f\"Initial Model Test Accuracy: {accuracy_initial:.4f}\")\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### TensorBoard for Initial Model\n",
        "# @markdown To view TensorBoard logs for the initial run, execute the following commands in a **separate Colab cell**:\n",
        "# @markdown ```\n",
        "# @markdown %load_ext tensorboard\n",
        "# @markdown %tensorboard --logdir logs/initial_fit\n",
        "# @markdown ```\n",
        "# @markdown ---\n",
        "\n",
        "# @title # 6. Addressing Overfitting\n",
        "print(\"\\n--- Addressing Overfitting ---\")\n",
        "# @markdown Looking at the 'Initial Model: Training and Validation Accuracy/Loss' plots above, we can observe:\n",
        "# @markdown * **Training Accuracy:** Steadily increases and likely reaches a high value.\n",
        "# @markdown * **Validation Accuracy:** Increases initially but then plateaus or even decreases, diverging from the training accuracy.\n",
        "# @markdown * **Training Loss:** Steadily decreases.\n",
        "# @markdown * **Validation Loss:** Decreases initially but then starts to increase, diverging from the training loss.\n",
        "# @markdown\n",
        "# @markdown This divergence is a classic sign of **overfitting**. The model is learning the training data *too* well, including its noise and specific patterns, and fails to generalize to new, unseen data (the validation set).\n",
        "# @markdown\n",
        "# @markdown **Techniques to Reduce Overfitting:**\n",
        "# @markdown 1.  **Data Augmentation:** Artificially increase the diversity of the training data by applying random transformations (rotation, zoom, flip, etc.) to the existing images. This helps the model learn more robust features.\n",
        "# @markdown 2.  **Dropout:** Randomly set a fraction of input units to 0 during training at each update step. This prevents units from co-adapting too much and forces the network to learn more redundant representations.\n",
        "# @markdown 3.  **(Optional) Transfer Learning:** Use a pre-trained model (like MobileNetV2, ResNet) trained on a large dataset (like ImageNet) and fine-tune it on our specific task. This leverages learned features and often leads to better performance with less data. (We will focus on Augmentation and Dropout here).\n",
        "\n",
        "# @title # 7. Implement Data Augmentation and Build Improved Model\n",
        "\n",
        "# Define data augmentation layers\n",
        "# These layers are active only during training.\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    # Add more augmentations if needed\n",
        "])\n",
        "\n",
        "# Build the improved model with Data Augmentation and Dropout\n",
        "improved_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)), # Input layer\n",
        "    data_augmentation, # Apply augmentation first\n",
        "    # Base Convolutional Layers (same as before or adjusted)\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    # Add Dropout before the dense layers\n",
        "    tf.keras.layers.Dropout(0.3), # Dropout rate of 30%\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3), # Another dropout layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # Output layer\n",
        "])\n",
        "\n",
        "# Compile the improved model\n",
        "# Often a slightly lower learning rate can be beneficial when using augmentation/dropout\n",
        "improved_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "improved_model.summary()\n",
        "\n",
        "# @title # 8. Train the Improved Model\n",
        "# Setup TensorBoard logs for the improved model\n",
        "log_dir_improved = os.path.join(\"logs\", \"improved_fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback_improved = tf.keras.callbacks.TensorBoard(log_dir=log_dir_improved, histogram_freq=1)\n",
        "\n",
        "# Define number of epochs for the improved model\n",
        "# Might need more epochs as augmentation makes training harder\n",
        "EPOCHS_IMPROVED = 25 # Adjust as needed\n",
        "\n",
        "print(\"\\n--- Training Improved Model (with Augmentation & Dropout) ---\")\n",
        "history_improved = improved_model.fit(\n",
        "    train_batches, # Augmentation layers handle the transformation on-the-fly\n",
        "    epochs=EPOCHS_IMPROVED,\n",
        "    validation_data=validation_batches,\n",
        "    callbacks=[tensorboard_callback_improved]\n",
        ")\n",
        "print(\"--- Improved Model Training Complete ---\")\n",
        "\n",
        "# @title # 9. Evaluate the Improved Model\n",
        "\n",
        "print(\"\\n--- Evaluating Improved Model ---\")\n",
        "# Plot training history\n",
        "plot_history(history_improved, title_prefix=\"Improved Model: \")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"Sample Predictions from Improved Model (on Test Set):\")\n",
        "show_sample_predictions(test_batches, improved_model, CLASS_NAMES)\n",
        "\n",
        "# Evaluate on the test set\n",
        "loss_improved, accuracy_improved = improved_model.evaluate(test_batches)\n",
        "print(f\"\\nImproved Model Test Loss: {loss_improved:.4f}\")\n",
        "print(f\"Improved Model Test Accuracy: {accuracy_improved:.4f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Initial Model Test Accuracy: {accuracy_initial:.4f}\")\n",
        "print(f\"Improved Model Test Accuracy: {accuracy_improved:.4f}\")\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### TensorBoard for Improved Model\n",
        "# @markdown To view TensorBoard logs for the improved run and compare with the initial run, execute the following commands in a **separate Colab cell**:\n",
        "# @markdown ```\n",
        "# @markdown %load_ext tensorboard\n",
        "# @markdown %tensorboard --logdir logs\n",
        "# @markdown ```\n",
        "# @markdown This will load logs from both the `initial_fit` and `improved_fit` directories, allowing direct comparison in the TensorBoard UI.\n",
        "# @markdown ---\n",
        "\n",
        "# @title # 10. Conclusion\n",
        "# @markdown We trained an initial CNN for cat vs. dog classification and observed overfitting.\n",
        "# @markdown By applying **Data Augmentation** and **Dropout**, we created an improved model.\n",
        "# @markdown Comparing the training curves and test accuracies, the improved model generally shows:\n",
        "# @markdown * Less divergence between training and validation metrics (reduced overfitting).\n",
        "# @markdown * Often, a higher final validation and test accuracy, indicating better generalization.\n",
        "# @markdown\n",
        "# @markdown Further improvements could involve:\n",
        "# @markdown * Trying different network architectures.\n",
        "# @markdown * Fine-tuning hyperparameters (learning rate, dropout rate, number of neurons/layers).\n",
        "# @markdown * Using **Transfer Learning** with a pre-trained model like MobileNetV2 or ResNet50V2."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ngdjH6a33290"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}